import streamlit as st
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.prompts import ChatPromptTemplate
import time
 
load_dotenv()
 
st.set_page_config(
    page_title="Aurora Skies Chatbot",
    page_icon="✈️",
    layout="centered",
    initial_sidebar_state="expanded"
)
 
with st.sidebar:
    st.title("About This App")
    st.markdown("""
    This is a RAG (Retrieval-Augmented Generation) chatbot for **Aurora Skies Airways**.
    
    It uses a provided FAQ dataset to answer your questions. The answers are generated by a Gemini LLM and are grounded in the FAQ context to prevent hallucinations.
    """)
    st.markdown("---")
    st.markdown("**Created by Vishal**")
 
PROMPT_TEMPLATE = """
You are an assistant for Aurora Skies Airways.
You must answer the user's question based *only* on the provided context.
 
If the context does not contain the answer, you must state:
'I'm sorry, I don't have information on that topic. For more help, please contact Aurora Skies Airways support.'
 
Do not make up information or use any external knowledge.
 
Context:
{context}
 
Question:
{input}
 
Answer:
"""
 
@st.cache_resource
def load_rag_chain():
    try:
        api_key = os.getenv("GOOGLE_API_KEY") 
        if not api_key:
            st.error("GOOGLE_API_KEY not found in .env file. Please create a .env file and add it.")
            return None
 
        llm = ChatGoogleGenerativeAI(
            model="models/gemini-1.5-flash",
            google_api_key=api_key
        )
 
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
 
        if not os.path.exists("faiss_index"):
            st.error("FAISS index not found. Please run 'python ingest.py' first!")
            return None
 
        db = FAISS.load_local(
            "faiss_index", 
            embeddings, 
            allow_dangerous_deserialization=True
        )
 
        retriever = db.as_retriever()
 
        prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
 
        document_chain = create_stuff_documents_chain(llm, prompt)
        rag_chain = create_retrieval_chain(retriever, document_chain)
        
        return rag_chain
 
    except Exception as e:
        st.error(f"Error initializing RAG chain: {e}")
        return None
 
st.title("✈️ Aurora Skies Airways Chatbot")
st.caption("Your AI assistant for booking, baggage, and flight info.")
 
rag_chain = load_rag_chain()
 
if rag_chain:
    chat_container = st.container()
    
    with chat_container:
        user_query = st.text_input("What's your question?", key="user_query")
 
        if user_query:
            with st.spinner("Finding an answer..."):
                response = rag_chain.invoke({"input": user_query})
                
                st.markdown("---")
                st.markdown("### Answer")
                st.markdown(response["answer"])
else:
    st.warning("Chatbot is not ready. Please run 'python ingest.py' and check your API key.")